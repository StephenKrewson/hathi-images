{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Find-all-pages-with-images-in-the-workset\" data-toc-modified-id=\"Find-all-pages-with-images-in-the-workset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Find all pages with images in the workset</a></span></li><li><span><a href=\"#Download-options-for-image-pages\" data-toc-modified-id=\"Download-options-for-image-pages-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Download options for image pages</a></span></li><li><span><a href=\"#Feature-reader-token-counts\" data-toc-modified-id=\"Feature-reader-token-counts-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature-reader token counts</a></span></li><li><span><a href=\"#Semantic-Similarity-Visualizations\" data-toc-modified-id=\"Semantic-Similarity-Visualizations-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Semantic Similarity Visualizations</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T18:39:30.195981Z",
     "start_time": "2018-03-27T18:39:30.152973Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c283ec0f8afa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mht_keys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mht\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatrixSimilarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "from config import ht_keys as ht\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "from hathidata.api import HathiDataClient\n",
    "from htrc_features import FeatureReader\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "%matplotlib notebook\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all pages with images in the workset\n",
    "\n",
    "After creating a collection on HathiTrust.org (while logged in), we can loop over the htids and find all the pages with images using the Data API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T18:39:30.257749Z",
     "start_time": "2018-03-27T18:39:30.201716Z"
    }
   },
   "outputs": [],
   "source": [
    "# establish a connection to the data API; Workset Tool only allows downloads in capsule\n",
    "access_key = ht['access_key']\n",
    "secret_key = ht['secret_key']\n",
    "data_api = HathiDataClient(access_key, secret_key)\n",
    "\n",
    "# project string name conventions:\n",
    "# HT METADATA is in         <project>_metadata.json\n",
    "# IMAGE_ON_PAGE list is in  <project>_images.json\n",
    "project = \"parley-america\"\n",
    "\n",
    "# Method 1: Download JSON metadata after creating collection on HathiTrust.org while logged in\n",
    "# My example collection is 328 19C books from Boston educational publisher Carter & Hendee\n",
    "# Note: you will need to rename this file using the convention above *outside* of the script\n",
    "metadata_path = project + \"_metadata.json\"\n",
    "\n",
    "with open(metadata_path, \"r\") as fp:\n",
    "    data = json.load(fp)\n",
    "vol_ids = [item['htitem_id'] for item in data['gathers']]\n",
    "\n",
    "# only query API if the JSON file does not exist\n",
    "dict_path = project + \"_images.json\"\n",
    "\n",
    "if not os.path.isfile(dict_path):\n",
    "    \n",
    "    # Dictionary keyed on htid stores image page lists\n",
    "    volumes = {}\n",
    "    \n",
    "    for i,vol in enumerate(vol_ids):\n",
    "        \n",
    "        # status update\n",
    "        print(i,vol)\n",
    "        \n",
    "        vol_meta = json.loads(data_api.request_vol_meta(vol, \"json\"))\n",
    "        sequence = vol_meta['htd:seqmap'][0]['htd:seq']\n",
    "        image_pages = [int(page['pseq']) for page in sequence if 'IMAGE_ON_PAGE' in page['htd:pfeat']]\n",
    "        \n",
    "        # add the list of image pages to the dictionary\n",
    "        volumes[vol] = image_pages\n",
    "        \n",
    "    # outside of the loop, save as json\n",
    "    with open(dict_path, \"w\") as fp:\n",
    "        json.dump(volumes, fp)\n",
    "        \n",
    "else:\n",
    "    with open(dict_path, \"r\") as fp:\n",
    "        volumes = json.load(fp)\n",
    "    \n",
    "\n",
    "print(\"Total number of images:\", sum([len(v) for k,v in volumes.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download options for image pages\n",
    "\n",
    "HathiTrust says that the Data API is feasible for up to 10k volumes. We are clearly well under. But it is still very time consuming to get ~7k page images along with a sliding three-page window for the raw OCR. The recommended solution is to ask HT for help in downloading a custom dataset with `rysnc`. See:\n",
    "\n",
    "- https://www.hathitrust.org/datasets\n",
    "- https://gist.github.com/lit-cs-sysadmin/8ffb90911697adc1262c\n",
    "\n",
    "However, for exploratory research, we want to be able to get started with the Data API. Our file structure will look like this:\n",
    "\n",
    "```\n",
    "<project>_metadata.json\n",
    "<project>_images.json\n",
    "\n",
    "<project>\n",
    "-->img\n",
    "   --><htid>\n",
    "      --><pg>.png\n",
    "         <pg>.png\n",
    "         ...\n",
    "      --><htid>\n",
    "         <pg>.png\n",
    "         <pg>.png\n",
    "         ...\n",
    "      ...\n",
    "-->ocr       \n",
    "   --><htid>\n",
    "      --><pg>.txt\n",
    "         <pg>.txt\n",
    "         ...\n",
    "      --><htid>\n",
    "         <pg>.txt\n",
    "         <pg>.txt\n",
    "         ...\n",
    "      ...       \n",
    "```\n",
    "        \n",
    "Practically speaking, this will take a few hours so I have broken the downloading code out into a script: `download_image_pages.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-reader token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T18:39:34.763660Z",
     "start_time": "2018-03-27T18:39:30.264741Z"
    }
   },
   "outputs": [],
   "source": [
    "# init feature-reader on volume list\n",
    "fr = FeatureReader(ids=vol_ids)\n",
    "\n",
    "# build up dictionary keyed on tokens, sum counts over all volumes\n",
    "counts = defaultdict(lambda: 0)\n",
    "for vol in fr:\n",
    "    VTL = vol.tokenlist(pages=False,case=False,pos=False,page_freq=False)\n",
    "    VTL = VTL.reset_index()\n",
    "    temp = VTL.to_dict('index')\n",
    "    for k,v in temp.items():\n",
    "        counts[v['lowercase']] += int(v['count'])\n",
    "\n",
    "# mappings for both directions: just use an incrementing index as the ID\n",
    "id2word = {}\n",
    "word2id = {}\n",
    "for i,k in enumerate(counts.keys()):\n",
    "    id2word[i] = k\n",
    "    word2id[k] = i\n",
    "    \n",
    "# additional brackets since from_corpus method takes a list argument\n",
    "corpus = [[(word2id[k],v) for k,v in counts.items()]]\n",
    "\n",
    "# gensim format, including the mapping\n",
    "dct = Dictionary.from_corpus(corpus, id2word=id2word)\n",
    "\n",
    "# flat list of page BoW lists (not grouped by volume)\n",
    "corpus_bows = []\n",
    "\n",
    "# nested list of (id,pg) lists for each volume\n",
    "corpus_idxs = []\n",
    "\n",
    "for i,vol in enumerate(fr):\n",
    "    # tokenlist from the entire volume, preserving page location info\n",
    "    PTL = vol.tokenlist(case=False,pos=False)\n",
    "    bow_idxs = []\n",
    "\n",
    "    for pg in volumes[vol.id]:\n",
    "        try:\n",
    "            bow = [(dct.token2id[k[1]],int(v['count'])) for k,v in PTL.loc[pg].to_dict('index').items()]\n",
    "            corpus_bows.append(bow)\n",
    "            bow_idxs.append((vol.year,pg))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    # convenient for knowing how many pages and which pages for each volume\n",
    "    corpus_idxs.append(bow_idxs)\n",
    "\n",
    "# create the model using BoWs over all volumes\n",
    "tfidf_model = TfidfModel(corpus_bows)\n",
    "\n",
    "# all pairwise term frequency-inverse document frequency similarities\n",
    "tfidf_sims = MatrixSimilarity(tfidf_model[corpus_bows], num_features=len(dct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T16:01:36.254244Z",
     "start_time": "2018-03-26T16:01:36.244448Z"
    }
   },
   "source": [
    "# Semantic Similarity Visualizations\n",
    "\n",
    "We use heatmaps of pairwise tf-idf similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T18:39:36.556768Z",
     "start_time": "2018-03-27T18:39:34.784205Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# top right block of the matrix is 0th vol vs 1st vol\n",
    "size1 = len(corpus_idxs[0])\n",
    "block1 = np.asarray(tfidf_sims)[:size1,size1:]\n",
    "\n",
    "# top right block of similarity matrix\n",
    "plt.figure()\n",
    "plt.title(\"1830 vs. 1827/1845 Tales about America (tf-idf similarity).\")\n",
    "sns.heatmap(block1,\n",
    "            xticklabels=corpus_idxs[1],\n",
    "            yticklabels=corpus_idxs[0],\n",
    "            vmax=0.5,\n",
    "            linewidths=0.5,\n",
    "            cmap=\"YlGnBu\")\n",
    "\n",
    "# we have to flatten the corpus_idx list since it is a list of lists\n",
    "plt.figure()\n",
    "plt.title(\"Full similarity matrix.\")\n",
    "sns.heatmap(tfidf_sims,\n",
    "            xticklabels=[itm for sublist in corpus_idxs for itm in sublist],\n",
    "            yticklabels=[itm for sublist in corpus_idxs for itm in sublist],\n",
    "            vmax=0.35,\n",
    "            linewidths=0.5,\n",
    "            cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
