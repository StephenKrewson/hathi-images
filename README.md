# HathiTrust Projects

- Scripts from 2016 Data Mining project as well as 2019 HTRC collaborative grant
- Instructions for accessing HT APIs

Ask D. Bamman about his code/findings:

https://www.hathitrust.org/htrc_sp17acs_awards


## Schedule

- [x] June 11: Project kickoff
- [x] June 25: Get/upload all 1800-1850 metadata from HathiFiles
- [ ] July 9: ???
- [ ] July 23: Sample list?


## Setup

Bibliographic metadata can be fetched from HTRC workset [toolkit](https://github.com/htrc/HTRC-WorksetToolkit). Recommended install is with [Anaconda](https://anaconda.org/pypi/htrc).

Hathi's UI for advanced catalog search is [here](https://catalog.hathitrust.org/Search/Advanced). You still cannot add to a collection from catalog search! This is very limiting.

Third-party wrappers: 

- Data, Bib, and Solr (search) APIs: https://github.com/rlmv/hathitrust-api
	- Now Python3 compatible; actively merging PRs
	- Install (with local `pip` from within `conda` environment): `pip install hathitrust-api`

Use `conda` environment `htrc`.


## Project Steps (2019)

### Generate Volume ID List

For 6/25/19 meeting, I need to present a list of HT volume IDs. "Education" is a decent subject heading--so is "History," which captures the Parley books--but many volumes (especially periodicals) lack any subject keywords.

There are many more comparative research questions (across formats and languages) that can be asked with a full sample. For 1800-1850, there are ~288,766 volumes. Since we are interested in serials and encyclopedias and similar printed formats, we would only want to exclude a few formats such as "Manuscript" and "CDROM." However, a more generalized way to do this is just to ignore volumes that do not contain a page sequence object.

**Warning!**

The 1800-1850 list can be generated ONLY by parsing the HathiFile (~1GB). This is because the Solr search API is completely deprecated--Hathi has made this clear! I wrote code to work with the HathiFile in `classify-image-junk` in the `src/sample.py` file.

Within this directory, run:

`python ids_by_date.py <START_DATE> <END_DATE> [<START-END.csv>]`

N.B. in my notebooks for the Yale DH Lab workshop the recommended way was using online HT search to generate JSON Collection file.

### Acquire Sample Pages

For 7/23/19 meeting, 


## Project Structure (2016)

N.B. this is not actively maintained.

file 	| purpose | note
--- 	| --- 		| ---
`books.txt` | list of books to download | one unique HT ID per line; generated by `ht_search_volumes.py`
`config.py` | stores HT API keys | not versioned, you'll need to get your own keys
`ht_download_images.py` | downloads all image pages for each book in `books.txt` | if a folder already exists with the name of an ID it is skipped; IDs that contain colons are skipped
`ht_search_volumes.py` | concatenates IDs returned from a bibliographic search onto `books.txt` | syntax for providing search fields (author, date, etc.) is given in a usage comment in the code
`ht_run_extraction.sh` | creates the `extracted` subfolder within all volume directories | skips volume if `extracted` already exists
`ht_classify_score.py` | from `extracted` subfolder, generates a corresponding Numpy array for each image in `vectors` sibling folder | adaptation of Inception modeal (2015)

