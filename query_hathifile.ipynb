{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early 19C Illustration Metadata: Final Report\n",
    "\n",
    "The my ACS project has successfully concluded with the creation of a large and novel dataset of illustration metadata. The dataset was produced in four stages using two specially retrained convolutional neural networks as well as one standard model (InceptionV3).\n",
    "\n",
    "The key deliverables of this projects are the following:\n",
    "\n",
    "- A csv file identifying all illustrated pages from HathiTrust volumes from the early 19C\n",
    "- A nearest-neighbors index (and utilities) for finding similar images to a \n",
    "\n",
    "I will discuss the four stages briefly before turning to a discussion and some examples. All listed files are included in the project's [Zenodo repository](TODO) unless stated otherwise. \n",
    "\n",
    "## Classification\n",
    "\n",
    "We began by identifying all Google-digitized volumes published during the years 1800-1850 (inclusive). These **500,013** volumes are contained in the file `google_ids_1800-1850.txt.gz`, which is a subset of the July 2019 [Hathifile](https://www.hathitrust.org/hathifiles). The Hathifile fields include basic publication information and are listed in `hathi_field_list.txt`.\n",
    "\n",
    "From this comprehensive set of early-nineteenth century volumes, we find all potentially illustrated pages using OCR-derived metadata. Apply a retrained CNN model to filter out noisy candidate pages.\n",
    "\n",
    "This model is built with Tensorflow and is located here: `model1`. Code for interacting with the model is here.\n",
    "\n",
    "My midpoint report describes the early steps in greater detail and can be found [here](https://wiki.htrc.illinois.edu/display/COM/A+Half-Century+of+Illustrated+Pages%3A+ACS+Lab+Notes).\n",
    "\n",
    "## Region of interest (ROI) extraction\n",
    "\n",
    "There **2,584,888** total ROIs. \n",
    "\n",
    "## Dimensionality reduction\n",
    "## Indexing and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Talk about users and applications as well as challenges (so much time for indexing steps). Future work. Need for image infrastructure and distributed workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the data\n",
    "\n",
    "This section presents Python code for working with the dataset. Note that HathiTrust APIs are used only sparingly. In general, it is much more efficient to download metadata in bulk and parse those files instead of making API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random, re, sys\n",
    "from annoy import AnnoyIndex\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the volumes used in the ACS project\n",
    "HATHIFILE = \"google_ids_1800-1850.txt.gz\"\n",
    "\n",
    "# corrected field names file. See also:\n",
    "# https://www.hathitrust.org/hathifiles_description\n",
    "HATHICOLS = \"hathifiles/hathi_field_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hathifile(ht_file, col_file):\n",
    "    \"\"\"\n",
    "    Return rows matching the query, as well as stubbytree paths for htids\n",
    "    \"\"\"\n",
    "\n",
    "    # Use iterative method to scale to full hathifiles\n",
    "    with open(col_file, \"r\") as fp:\n",
    "        col_names = fp.readline().strip('\\n').split('\\t')\n",
    "        num_cols = len(col_names)\n",
    "\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    iter_csv = pd.read_csv(\n",
    "        ht_file, \n",
    "        sep='\\t', \n",
    "        header=None,\n",
    "        names=col_names,\n",
    "        engine='c',\n",
    "        # quicker if we can assert some types for the fields\n",
    "        dtype={\n",
    "            'htid': 'str',\n",
    "            'rights_date_used': 'object',\n",
    "            'pub_place': 'str', # sadly, this is just the partner lib\n",
    "            'imprint': 'str'\n",
    "        },\n",
    "        iterator=True,\n",
    "        chunksize=5000,\n",
    "        error_bad_lines=False)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for i, chunk in enumerate(iter_csv):\n",
    "\n",
    "        # hard code query: use a basic regex with matching group\n",
    "        # find: \"Munroe, Francis\", \"Munroe and Francis\", \"Munroe & Francis\"\n",
    "        conditions = (chunk['imprint'].str.contains(\n",
    "            r\"\\bMunroe(?:,| and| &) Francis\\b\",\n",
    "            na=False,\n",
    "            flags=re.IGNORECASE)\n",
    "        )\n",
    "        # concatenate valid rows, idx doesn't matter\n",
    "        df = pd.concat([df, chunk[conditions]], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = search_hathifile(HATHIFILE, HATHICOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 26)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions from Hathi's feature datasets\n",
    "# https://github.com/htrc/htrc-feature-reader/blob/39010fd41c049f4f86b9c8ff4a44e000217093c2/htrc_features/utils.py\n",
    "\n",
    "def _id_encode(id):\n",
    "    '''\n",
    "    :param id: A Pairtree ID. If it's a Hathitrust ID, this is the part after the library\n",
    "        code; e.g. the part after the first period for vol.123/456.\n",
    "    :return: A sanitized id. e.g., 123/456 will return as 123=456 to avoid filesystem issues.\n",
    "    '''\n",
    "    return id.replace(\":\", \"+\").replace(\"/\", \"=\").replace(\".\", \",\")\n",
    "\n",
    "def _id_decode(id):\n",
    "    '''\n",
    "    :param id: A sanitized Pairtree ID.\n",
    "    :return: An original Pairtree ID.\n",
    "    '''\n",
    "    return id.replace(\"+\", \":\").replace(\"=\", \"/\").replace(\",\", \".\")\n",
    "\n",
    "def clean_htid(htid):\n",
    "    '''\n",
    "    :param htid: A HathiTrust ID of form lib.vol; e.g. mdp.1234\n",
    "    :return: A sanitized version of the HathiTrust ID, appropriate for filename use.\n",
    "    '''\n",
    "    libid, volid = htid.split('.', 1)\n",
    "    volid_clean = _id_encode(volid)\n",
    "    return '.'.join([libid, volid_clean])\n",
    "\n",
    "def id_to_stubbytree(htid, format = None, suffix = None, compression = None):\n",
    "    '''\n",
    "    Take an HTRC id and convert it to a 'stubbytree' location.\n",
    "    '''\n",
    "    libid, volid = htid.split('.', 1)\n",
    "    volid_clean = _id_encode(volid)\n",
    "\n",
    "    suffixes = [s for s in [format, compression] if s is not None]\n",
    "    filename = \".\".join([clean_htid(htid), *suffixes])\n",
    "    path = os.path.join(libid, volid_clean[::3], filename)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stubby_ids = [id_to_stubbytree(htid) for htid in df.htid.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stubby_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mdp\\\\31331\\\\mdp.39015038731918',\n",
       " 'mdp\\\\31197\\\\mdp.39015010791476',\n",
       " 'mdp\\\\31198\\\\mdp.39015010791484',\n",
       " 'mdp\\\\31199\\\\mdp.39015010791492',\n",
       " 'mdp\\\\31190\\\\mdp.39015010791500']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stubby_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_dir = Path(os.path.abspath('../_app-files/roi-vectors/vectors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each volume, find associated .npy vectors within stubbytree directory\n",
    "munroe_francis = {}\n",
    "\n",
    "for stubby_id in stubby_ids:\n",
    "    vol_vectors = glob(os.path.join(rois_dir, stubby_id + \"*.npy\"))\n",
    "    if len(vol_vectors) != 0:\n",
    "        munroe_francis[stubby_id] = vol_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for vec_list in munroe_francis.values():\n",
    "    total += len(vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1477"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1477 vectors from 118 illustrated volumes (out of 360 total)\n",
    "# dimensionality of the vectors is (1,1000) per np.shape\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('munroe.csv', 'w') as fp:\n",
    "    for stubby_id in munroe_francis.keys():\n",
    "        fp.write(os.path.normpath(stubby_id))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified from: https://github.com/spotify/annoy\n",
    "\n",
    "f = 1000\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "i = 0\n",
    "\n",
    "# Find all vectors per volume and index them from 0\n",
    "for k,v in munroe_francis.items():\n",
    "    for vec in v:\n",
    "        item = np.load(vec)\n",
    "        # transpose vector since it needs to be (1000,1) not (1,1000)\n",
    "        t.add_item(i, item.T)\n",
    "        i += 1\n",
    "\n",
    "# Try with 1000 trees\n",
    "t.build(1000)\n",
    "t.save('test.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 614, 979, 905, 968, 177, 900, 955, 893, 677]\n"
     ]
    }
   ],
   "source": [
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('test.ann')\n",
    "print(u.get_nns_by_item(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# great news! building that tree took about 20s on my laptop\n",
    "# really need to have data frame with index for each vector; keep name similar to .ann file\n",
    "# say t-SNE and all that can be future directions (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is C2C5-01EE\n",
      "\n",
      " Directory of C:\\Users\\stephen-krewson\\Documents\\_app-files\n",
      "\n",
      "07/28/2020  03:33 PM    <DIR>          .\n",
      "07/28/2020  03:33 PM    <DIR>          ..\n",
      "08/21/2019  04:13 PM    <DIR>          19c-book-illustrations\n",
      "08/21/2019  04:24 PM    12,357,353,554 19c-book-illustrations-2.zip\n",
      "08/03/2019  02:54 PM    12,359,693,970 19c-book-illustrations.zip\n",
      "11/05/2018  01:50 PM       234,493,010 combined_data.json\n",
      "10/02/2018  02:53 PM        17,152,239 default_graph.pb\n",
      "06/29/2019  10:14 AM            27,142 Dissertation.zip\n",
      "07/28/2020  03:30 PM    18,015,031,444 early-19C-illustrations_full-index.ann\n",
      "07/28/2020  12:54 PM        11,207,602 early-19C-illustrations_full-index_list.txt.gz\n",
      "06/29/2019  10:17 AM            13,054 evangelical-black-atlantic-report.zip\n",
      "11/25/2018  03:15 PM       298,068,462 FULL_mhl_1770-1879.json\n",
      "11/10/2018  12:50 PM       265,513,072 FULL_mhl_1800-1879.json\n",
      "07/03/2019  04:13 PM    <DIR>          HUMS 304b [2015]\n",
      "07/23/2018  01:52 PM               519 keys.py\n",
      "10/02/2018  02:54 PM       255,858,144 mask_rcnn_balloon_0030.h5\n",
      "02/02/2020  09:37 PM       255,858,144 mask_rcnn_bbox_weights.h5\n",
      "11/26/2018  04:51 PM       340,468,819 MERGED_mhl_1770-1879.json\n",
      "02/16/2019  04:15 PM    <DIR>          mhl-sample\n",
      "11/09/2018  07:27 PM       114,329,233 mhl_1800-1849_AUGMENTED.json\n",
      "09/27/2018  08:42 PM       109,971,188 mhl_1800-1849_OLD.json\n",
      "07/24/2020  06:45 PM    <DIR>          roi-vectors\n",
      "08/03/2019  02:37 PM    <DIR>          training_imgs\n",
      "              16 File(s) 44,635,039,596 bytes\n",
      "               7 Dir(s)  80,679,649,280 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir ..\\_app-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try the full index... it is VERY fast!\n",
    "# It's just that build the index is linear: https://markroxor.github.io/gensim/static/notebooks/annoytutorial.html\n",
    "u2 = AnnoyIndex(f, 'angular')\n",
    "u2.load('../_app-files/early-19C-illustrations_full-index.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('..\\_app-files\\early-19C-illustrations_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htid_page_seq_nns_metadata(htid, seq_num, nns_index, df_meta, k):\n",
    "    \"\"\"Given a target htid:page_seq and metadata dataframe, gives metadata for k neighboring images\"\"\"\n",
    "    \n",
    "    # get the index for the page in question (N.B. whitespace in column name)\n",
    "    idx = df_meta[(df_meta['htid'] == htid) & (df_meta['page_seq'] == seq_num)].index\n",
    "    \n",
    "    # multiple crops alert\n",
    "    if len(idx) > 1:\n",
    "        print(\"Multiple crops for this page_seq\")\n",
    "        \n",
    "    # the nearest neighbor ROI indices\n",
    "    nns = nns_index.get_nns_by_item(idx[0], k)\n",
    "    \n",
    "    print(nns)\n",
    "    \n",
    "    # return rows from metadat table matching these indices\n",
    "    return df_meta.iloc[nns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1617907, 254046, 1613603, 1269805, 1844016, 1341309, 1724782, 1407896, 2438718, 1454741]\n",
      "                        htid  page_seq    page_label  crop_no  \\\n",
      "1617907  uiug.30112003448526        28  inline_image        0   \n",
      "254046        ucm.5321309033       347  inline_image        0   \n",
      "1613603  uiug.30112048888058       326  inline_image        0   \n",
      "1269805        chi.097881099        75  inline_image        0   \n",
      "1844016       uc1.c046857802       166  inline_image        1   \n",
      "1341309         chi.79355181       551  inline_image        0   \n",
      "1724782         uc1.$b557159       190   plate_image        0   \n",
      "1407896   njp.32101063578338        46   plate_image        0   \n",
      "2438718           hvd.hn5cxz        19  inline_image        0   \n",
      "1454741   njp.32101080155110        80   plate_image        0   \n",
      "\n",
      "                                            vector_path  \n",
      "1617907  uiug/31042/uiug.30112003448526_00000028_00.npy  \n",
      "254046          ucm/5193/ucm.5321309033_00000347_00.npy  \n",
      "1613603  uiug/31485/uiug.30112048888058_00000326_00.npy  \n",
      "1269805           chi/080/chi.097881099_00000075_00.npy  \n",
      "1844016         uc1/c672/uc1.c046857802_00000166_01.npy  \n",
      "1341309            chi/758/chi.79355181_00000551_00.npy  \n",
      "1724782            uc1/$55/uc1.$b557159_00000190_00.npy  \n",
      "1407896    njp/30673/njp.32101063578338_00000046_00.npy  \n",
      "2438718               hvd/hc/hvd.hn5cxz_00000019_00.npy  \n",
      "1454741    njp/30851/njp.32101080155110_00000080_00.npy  \n"
     ]
    }
   ],
   "source": [
    "# explain how this stuff can be found through HT viewer! it's in the URL\n",
    "# end writeup with rousing call for this metadata to be derived for ALL hathi years + every time a new\n",
    "# set of scans gets uploaded\n",
    "univ_history = htid_page_seq_nns_metadata('uiug.30112003448526', 28, u2, df_meta, 10)\n",
    "print(univ_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: ask Boris if htids are encoded with the standard utility methods\n",
    "# In the meta_csv, htids are NOT encoded; in vectors.txt and anywhere they are in file format, they ARE encoded\n",
    "\n",
    "\n",
    "# find way to look up TITLE of work\n",
    "# do montage of ENTIRE publisher set (really want t-SNE, but maybe Damon has something like that?)\n",
    "# don't even need vectors, per se, for t-SNE on tractable subsets\n",
    "# compare Hawthorne ballon (my McNeil paper) with Parley's moon and stars (sadly, added too late to make project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Thank you Ryan and Boris, especially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Project assets\n",
    "\n",
    "The deliverables are available via Zenodo:\n",
    "\n",
    "- A csv file with basic metadata\n",
    "- A nearest-neighbors index\n",
    "\n",
    " The code used for the data processing is on [GitHub](https://github.com/htrc/ACS-krewson)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
