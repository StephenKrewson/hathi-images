{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize all illustrations for a given query\n",
    "\n",
    "The project data can be unwieldy to work with. In many cases, you want to isolate a subset of the 2.5+ million illustrated regions. Analysis can then be done at a smaller scale and more quickly. This notebook shows how to get started with such research. The goal will be to find the metadata for all books published in 1800-1850 by the Boston firm Munroe & Francis.\n",
    "\n",
    "1. To create a set of volume IDs to visualize, search Hathifile excerpt using metadata query\n",
    "2. Using volume IDs from Step 1, find all illustrated pages corresponding to those volumes. Need to do this with a file IN the Zenodo or that mimics the Google Cloud bucket.\n",
    "3. Create a PixPlot metadata file that joins the Hathifile information with the stubby IDs.\n",
    "4. Get image assets from cloud storage\n",
    "5. Run PixPlot processing\n",
    "6. Visualize with PixPlot instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Filter Hathifile using metadata; extract matching rows\n",
    "\n",
    "Hathifiles can be very big, so we iteratively search them for field (column) values matching a query. This can take some finesse, since publisher names are often very similar and the name of a firm can be written in slightly different ways (e.g. '&' vs. 'and')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random, re, sys\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that we need to open/access to generate the PixPlot metadata file. It's simplest to use absolute paths.\n",
    "# These files are available in the Zenodo repository: http://zenodo.org/record/3940528#.XyRNSZ5KjIU\n",
    "\n",
    "# 1. Hathifile subset, for performing basic metadata queries\n",
    "HATHIFILE = \"/home/stephen-krewson/project-hathi-images/datafiles/google_ids_1800-1850.txt.gz\"\n",
    "HATHICOLS = \"/home/stephen-krewson/project-hathi-images/datafiles/hathi_field_list.txt\"\n",
    "\n",
    "# 2. Flat file of all 2.5m \"regions of interest\" (illustrations)\n",
    "# TODO: should I use gzip on this?\n",
    "ROIFILE = \"/home/stephen-krewson/project-hathi-images/datafiles/early-19C-illustrations_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File: /home/stephen-krewson/project-hathi-images/datafiles/google_ids_1800-1850.txt.gz\n",
      "  Size: 30724746  \tBlocks: 60016      IO Block: 4096   regular file\n",
      "Device: 810h/2064d\tInode: 29532       Links: 1\n",
      "Access: (0644/-rw-r--r--)  Uid: ( 1000/stephen-krewson)   Gid: ( 1000/stephen-krewson)\n",
      "Access: 2021-04-07 08:38:14.840310100 -0400\n",
      "Modify: 2019-07-31 12:07:38.271850000 -0400\n",
      "Change: 2021-04-07 08:38:01.500310100 -0400\n",
      " Birth: -\n",
      "  File: /home/stephen-krewson/project-hathi-images/datafiles/hathi_field_list.txt\n",
      "  Size: 307       \tBlocks: 8          IO Block: 4096   regular file\n",
      "Device: 810h/2064d\tInode: 56569       Links: 1\n",
      "Access: (0644/-rw-r--r--)  Uid: ( 1000/stephen-krewson)   Gid: ( 1000/stephen-krewson)\n",
      "Access: 2021-04-07 08:02:40.270310100 -0400\n",
      "Modify: 2021-04-07 08:01:30.360310100 -0400\n",
      "Change: 2021-04-07 08:01:30.360310100 -0400\n",
      " Birth: -\n",
      "  File: /home/stephen-krewson/project-hathi-images/datafiles/early-19C-illustrations_metadata.csv\n",
      "  Size: 202726013 \tBlocks: 395952     IO Block: 4096   regular file\n",
      "Device: 810h/2064d\tInode: 42248       Links: 1\n",
      "Access: (0644/-rw-r--r--)  Uid: ( 1000/stephen-krewson)   Gid: ( 1000/stephen-krewson)\n",
      "Access: 2021-04-08 08:40:38.887452800 -0400\n",
      "Modify: 2020-07-29 20:43:39.064327300 -0400\n",
      "Change: 2021-04-08 08:40:40.227452800 -0400\n",
      " Birth: -\n"
     ]
    }
   ],
   "source": [
    "# Test that the files exist\n",
    "!stat $HATHIFILE\n",
    "!stat $HATHICOLS\n",
    "!stat $ROIFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hathifile(ht_file, col_file, search_col, search_expr):\n",
    "    '''\n",
    "    :param ht_file: A Hathifile in CSV format.\n",
    "    :param col_file: A newline-delimited file with the Hathifile column names\n",
    "    :param search_col: The field/column on which to search\n",
    "    :param search_expr: A regular expression against which search_col values can be compared\n",
    "    :return: A pandas dataframe of rows in which search_col matches search_expr\n",
    "    '''\n",
    "    # Use iterative method to scale to full hathifiles\n",
    "    with open(col_file, \"r\") as fp:\n",
    "        col_names = fp.readline().strip('\\n').split('\\t')\n",
    "        num_cols = len(col_names)\n",
    "\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    iter_csv = pd.read_csv(\n",
    "        ht_file, \n",
    "        sep='\\t', \n",
    "        header=None,\n",
    "        names=col_names,\n",
    "        engine='c',\n",
    "        # quicker if we can assert some types for the fields\n",
    "        dtype={\n",
    "            'htid': 'str',\n",
    "            'rights_date_used': 'object', # values NOT guaranteed to be numeric\n",
    "            'pub_place': 'str', # sadly, this is just the partner lib\n",
    "            'imprint': 'str'\n",
    "        },\n",
    "        iterator=True,\n",
    "        chunksize=5000,\n",
    "        error_bad_lines=False)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for i, chunk in enumerate(iter_csv):\n",
    "        condition = (chunk[search_col].str.contains(search_expr, na=False, flags=re.IGNORECASE))\n",
    "        \n",
    "        # hathifile idx has no relation to Neighbor tree: ignore\n",
    "        df = pd.concat([df, chunk[condition]], ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 26)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_col = 'imprint'\n",
    "\n",
    "# find publishers \"Munroe, Francis\", \"Munroe and Francis\", \"Munroe & Francis\" (with matching group)\n",
    "# search_expr = r\"\\bMunroe(?:,| and| &) Francis\\b\"\n",
    "# search_label = \"munroe-francis\"\n",
    "\n",
    "# similar string, but for carter and hendee\n",
    "search_expr = r\"\\bCarter(?:,| and| &) Hendee\\b\"\n",
    "# a label for the results of this experiment (in case you want to compare later)\n",
    "search_label = \"carter-hendee\"\n",
    "\n",
    "df = search_hathifile(HATHIFILE, HATHICOLS, search_col, search_expr)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['htid', 'access', 'rights', 'ht_bib_key', 'description', 'source',\n",
       "       'source_bib_num', 'oclc_num', 'isbn', 'issn', 'lccn', 'title',\n",
       "       'imprint', 'rights_reason_code', 'rights_timestamp', 'us_gov_doc_flag',\n",
       "       'rights_date_used', 'pub_place', 'lang', 'bib_fmt', 'collection_code',\n",
       "       'content_provider_code', 'responsible_entity_code',\n",
       "       'digitization_agent_code', 'access_profile_code', 'author'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns # use title, rights_date_used, imprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date objects to integers, for the year of publication\n",
    "df['rights_date_used'] = pd.to_numeric(df['rights_date_used']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imprint</th>\n",
       "      <th>title</th>\n",
       "      <th>rights_date_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Carter, Hendee, and Co. 1836.</td>\n",
       "      <td>The mercantile arithmetic, adapted to the comm...</td>\n",
       "      <td>1833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Carter, Hendee, 1832.</td>\n",
       "      <td>Sermons and charges / by James Freeman.</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Carter, Hendee and Co., 1832</td>\n",
       "      <td>Rudiments of the Italian language, or, Easy le...</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Carter, Hendee, 1832.</td>\n",
       "      <td>Sermons and charges / by James Freeman.</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Carter, Hendee &amp; Babcock, l83l.</td>\n",
       "      <td>The buckwheat cake, a poem ..</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Carter, Hendee &amp; co., 1832.</td>\n",
       "      <td>The biographies of Lady Russell, and Madame Gu...</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Peirce and Parker, and Carter and Hendee], 183...</td>\n",
       "      <td>The Naturalist</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Carter and Hendee, 1829.</td>\n",
       "      <td>The constitution of man considered in relation...</td>\n",
       "      <td>1829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Carter, Hendee, 1833.</td>\n",
       "      <td>A manual containing information respecting the...</td>\n",
       "      <td>1833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Hilliard Gray and co., and Carter, Hendee and ...</td>\n",
       "      <td>Adam's Latin grammar, with some improvements, ...</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               imprint  \\\n",
       "224                      Carter, Hendee, and Co. 1836.   \n",
       "150                              Carter, Hendee, 1832.   \n",
       "237                       Carter, Hendee and Co., 1832   \n",
       "152                              Carter, Hendee, 1832.   \n",
       "239                    Carter, Hendee & Babcock, l83l.   \n",
       "58                         Carter, Hendee & co., 1832.   \n",
       "63   Peirce and Parker, and Carter and Hendee], 183...   \n",
       "97                            Carter and Hendee, 1829.   \n",
       "78                               Carter, Hendee, 1833.   \n",
       "45   Hilliard Gray and co., and Carter, Hendee and ...   \n",
       "\n",
       "                                                 title  rights_date_used  \n",
       "224  The mercantile arithmetic, adapted to the comm...              1833  \n",
       "150            Sermons and charges / by James Freeman.              1832  \n",
       "237  Rudiments of the Italian language, or, Easy le...              1832  \n",
       "152            Sermons and charges / by James Freeman.              1832  \n",
       "239                      The buckwheat cake, a poem ..              1831  \n",
       "58   The biographies of Lady Russell, and Madame Gu...              1832  \n",
       "63                                      The Naturalist              1831  \n",
       "97   The constitution of man considered in relation...              1829  \n",
       "78   A manual containing information respecting the...              1833  \n",
       "45   Adam's Latin grammar, with some improvements, ...              1832  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show a few results -- just the search field and the date published\n",
    "df[[search_col, 'title', 'rights_date_used']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Construct page image IDs for all htids in the query results\n",
    "\n",
    "We have a bunch of `htid`s from the Hathifile, but many of them will not contain any illustrations. To narrow down our set of results, we need to look up the `htid`s in our illustration metadata. This can be done with the main CSV file (???) or with the vectors.tar file.\n",
    "\n",
    "Since the dataset is stored using a stubby tree, we need utilities for working with these paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions from Hathi's feature datasets\n",
    "# https://github.com/htrc/htrc-feature-reader/blob/39010fd41c049f4f86b9c8ff4a44e000217093c2/htrc_features/utils.py\n",
    "def _id_encode(id):\n",
    "    '''\n",
    "    :param id: A Pairtree ID. If it's a Hathitrust ID, this is the part after the library\n",
    "        code; e.g. the part after the first period for vol.123/456.\n",
    "    :return: A sanitized id. e.g., 123/456 will return as 123=456 to avoid filesystem issues.\n",
    "    '''\n",
    "    return id.replace(\":\", \"+\").replace(\"/\", \"=\").replace(\".\", \",\")\n",
    "\n",
    "def _id_decode(id):\n",
    "    '''\n",
    "    :param id: A sanitized Pairtree ID.\n",
    "    :return: An original Pairtree ID.\n",
    "    '''\n",
    "    return id.replace(\"+\", \":\").replace(\"=\", \"/\").replace(\",\", \".\")\n",
    "\n",
    "def clean_htid(htid):\n",
    "    '''\n",
    "    :param htid: A HathiTrust ID of form lib.vol; e.g. mdp.1234\n",
    "    :return: A sanitized version of the HathiTrust ID, appropriate for filename use.\n",
    "    '''\n",
    "    libid, volid = htid.split('.', 1)\n",
    "    volid_clean = _id_encode(volid)\n",
    "    return '.'.join([libid, volid_clean])\n",
    "\n",
    "def id_to_stubbytree(htid, format = None, suffix = None, compression = None):\n",
    "    '''\n",
    "    Take an HTRC id and convert it to a 'stubbytree' location.\n",
    "    '''\n",
    "    libid, volid = htid.split('.', 1)\n",
    "    volid_clean = _id_encode(volid)\n",
    "\n",
    "    suffixes = [s for s in [format, compression] if s is not None]\n",
    "    filename = \".\".join([clean_htid(htid), *suffixes])\n",
    "    path = os.path.join(libid, volid_clean[::3], filename)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the stubbytree dir paths to the original htids\n",
    "stubby_dict = {htid: id_to_stubbytree(htid) for htid in df.htid.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvd.hnztud --> hvd/ht/hvd.hnztud\n"
     ]
    }
   ],
   "source": [
    "# Print the intitial htid and show its stubby id transformation\n",
    "for k,v in stubby_dict.items():\n",
    "    print(k, \"-->\", v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The project CSV is 200 MB... can I gzip it? And do an efficient join on htid with pandas?\n",
    "# Fields are: htid, page_seq, page_label, crop_no, vector_path\n",
    "# See: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old method: use Unix globbing to find stubby .npy files and swap out the extension\n",
    "# Problem: Vectors were a dead end, so we want to exclusively work with flat metadata files\n",
    "\n",
    "# for each volume, find associated .npy vectors within stubbytree directory -- store in dictionary\n",
    "\n",
    "#VEC_DIR = os.path.abspath(\"../_app-files/roi-vectors/vectors\")\n",
    "\n",
    "#query_vectors = {}\n",
    "\n",
    "#for stubby_id in stubby_dict.keys():\n",
    "#    vol_path = os.path.join(VEC_DIR, stubby_id + \"*.npy\")\n",
    "#    vol_vectors = glob(vol_path)\n",
    "#    if len(vol_vectors) != 0:\n",
    "#        query_vectors[stubby_id] = vol_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reformat ROIs with metadata for Pixplot\n",
    "\n",
    "We can reformat our selected ROIs, taking selected columns and renaming them. If we are able to acquire image data, this will allow us to attach the metadata and build a PixPlot visualization.\n",
    "\n",
    "See https://github.com/YaleDHLab/pix-plot for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns we want to keep from hathifile: these will map to 'description' and 'year' in PixPlot's format\n",
    "col_map = {\n",
    "    'rights_date_used': 'year',\n",
    "    'title': 'description'\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for k, v in query_vectors.items():\n",
    "    \n",
    "    # transform .npy file into jpeg, separate from rest of path\n",
    "    for npy_file in v:\n",
    "        \n",
    "        vec_base = os.path.basename(npy_file)\n",
    "        img_base = os.path.splitext(vec_base)[0] + '.jpg'\n",
    "        \n",
    "        # remember the unencoded htid\n",
    "        htid = stubby_dict[k]\n",
    "        \n",
    "        # row to be added to df_pixplot\n",
    "        row = {}\n",
    "        \n",
    "        # get metadata for this volume\n",
    "        metadata = df[df['htid'] == htid][col_map.keys()]\n",
    "        \n",
    "        # tricky, since values could be a list or object\n",
    "        for col in metadata.columns:\n",
    "            row[col_map[col]] = metadata[col].values[0]\n",
    "\n",
    "        # add img_base path and label\n",
    "        row['filename'] = img_base\n",
    "        row['label'] = search_label\n",
    "        \n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>description</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1832</td>\n",
       "      <td>Les contes de Pierre Parley sur l'Amérique--</td>\n",
       "      <td>nyp.33433012110858_00000088_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>1832</td>\n",
       "      <td>Les contes de Pierre Parley sur l'Amérique--</td>\n",
       "      <td>nyp.33433012110858_00000079_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>1830</td>\n",
       "      <td>Youth's keepsake.</td>\n",
       "      <td>nyp.33433105233252_00000160_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1850</td>\n",
       "      <td>The Farmer's almanack.</td>\n",
       "      <td>hvd.32044044506970_00000461_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1830</td>\n",
       "      <td>The young lady's book : a manual of elegant re...</td>\n",
       "      <td>uc1.$b264661_00000076_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1835</td>\n",
       "      <td>Elements of natural philosophy, with questions...</td>\n",
       "      <td>njp.32101013012156_00000254_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1834</td>\n",
       "      <td>Scientific tracts.</td>\n",
       "      <td>umn.319510027996728_00000018_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>1831</td>\n",
       "      <td>The architecture of birds.</td>\n",
       "      <td>nyp.33433011003872_00000089_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>1833</td>\n",
       "      <td>Scenes of American wealth and industry in prod...</td>\n",
       "      <td>nyp.33433007272812_00000039_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>1830</td>\n",
       "      <td>Youth's keepsake.</td>\n",
       "      <td>nyp.33433105233252_00000078_00.jpg</td>\n",
       "      <td>carter-hendee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                        description  \\\n",
       "739   1832       Les contes de Pierre Parley sur l'Amérique--   \n",
       "734   1832       Les contes de Pierre Parley sur l'Amérique--   \n",
       "2469  1830                                  Youth's keepsake.   \n",
       "2128  1850                             The Farmer's almanack.   \n",
       "177   1830  The young lady's book : a manual of elegant re...   \n",
       "58    1835  Elements of natural philosophy, with questions...   \n",
       "1979  1834                                 Scientific tracts.   \n",
       "760   1831                         The architecture of birds.   \n",
       "834   1833  Scenes of American wealth and industry in prod...   \n",
       "2462  1830                                  Youth's keepsake.   \n",
       "\n",
       "                                 filename          label  \n",
       "739    nyp.33433012110858_00000088_00.jpg  carter-hendee  \n",
       "734    nyp.33433012110858_00000079_00.jpg  carter-hendee  \n",
       "2469   nyp.33433105233252_00000160_00.jpg  carter-hendee  \n",
       "2128   hvd.32044044506970_00000461_00.jpg  carter-hendee  \n",
       "177          uc1.$b264661_00000076_00.jpg  carter-hendee  \n",
       "58     njp.32101013012156_00000254_00.jpg  carter-hendee  \n",
       "1979  umn.319510027996728_00000018_00.jpg  carter-hendee  \n",
       "760    nyp.33433011003872_00000089_00.jpg  carter-hendee  \n",
       "834    nyp.33433007272812_00000039_00.jpg  carter-hendee  \n",
       "2462   nyp.33433105233252_00000078_00.jpg  carter-hendee  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn dict rows into dataframe -- 'filename' shows the convention for image paths used in the project\n",
    "df_pixplot = pd.DataFrame.from_dict(rows)\n",
    "df_pixplot.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the search label to make a metadata path\n",
    "#metadata_csv = \"{}_pixplot-metadata.csv\".format(search_label)\n",
    "\n",
    "# save as a CSV that PixPlot can accept\n",
    "#df_pixplot.to_csv(metadata_csv, sep=',', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
