<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Stephen W. Krewson" />
  <title>Derived Metadata for Early 19C Illustrations: ACS Grant Final Report</title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<div id="header">
<h1 class="title">Derived Metadata for Early 19C Illustrations: ACS Grant Final Report</h1>
<h2 class="author">Stephen W. Krewson</h2>
<h3 class="date">August 3, 2020</h3>
</div>
<div id="TOC">
<ul>
<li><a href="#early-19c-illustration-metadata-final-report">Early 19C Illustration Metadata: Final Report</a><ul>
<li><a href="#project-assets">Project assets</a></li>
<li><a href="#discussion">Discussion</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
</ul></li>
</ul>
</div>
<div id="early-19c-illustration-metadata-final-report" class="section level1">
<h1>Early 19C Illustration Metadata: Final Report</h1>
<p>The “Deriving Basic Illustration Metadata” project has successfully concluded with the creation of a large and novel dataset of illustration metadata. The dataset was produced in four stages using two retrained convolutional neural networks as well as one standard model (InceptionV3).</p>
<p>The stages were as follows:</p>
<ol style="list-style-type: decimal">
<li><strong>[Find illustrated pages]</strong> Identify all Google-digitized volumes published during the years 1800-1850 (inclusive). From this set of 500,013 volumes, use OCR metadata to find pages likely to contain illustrations. Classify each candidate page with a retrained CNN and keep pages labeled with <code>inline_image</code> and <code>plate_image</code>. My midpoint report describes this stage in greater detail and can be found <a href="https://wiki.htrc.illinois.edu/display/COM/A+Half-Century+of+Illustrated+Pages%3A+ACS+Lab+Notes">here</a>.</li>
<li><strong>[Extract illustrated regions]</strong> Mask-RCNN models slide a window across an image input looking for regions that activate a target or “foreground” class. Regions that activate past a certain threshold are estimated to be regions of interest (ROIs). The rest of the image is considered to be the “background.” Using a Mask-RCNN model retrained with annotated 19C page images (i.e. bounding boxes around illustrated regions), we extracted more than 2.5 million ROIs from the pages identified in Stage 1.</li>
<li><strong>[Generate lower-dimensional representations]</strong> Comparing images programmatically requires lower-dimensional numerical representations. We used the InceptionV3 CNN to turn each ROI JPEG into a 1000-dimensional vector. This allows a distance metric to be applied pairwise to any two images in the dataset.</li>
<li><strong>[Build indices and visualize]</strong> We used the <a href="https://github.com/spotify/annoy">Annoy library</a> from Spotify to build an index of (approximate) nearest neighbors from the vector representations generated in Stage 3. This index allows calculation of the <em>k</em> most similar image vectors to an input image vector. Notebooks for using and building Annoy indexes have been provided.</li>
</ol>
<p>The key deliverables of this projects are the following:</p>
<ul>
<li>A CSV file identifying all illustrated regions from HathiTrust volumes published between 1800 and 1850</li>
<li>A nearest-neighbors index created from vector representations of these 2.5M regions of interest (ROIs)</li>
<li>Sample notebooks for working with the metadata and index files</li>
</ul>
<p>The metadata and index files are included in the project’s <a href="https://zenodo.org/record/3940528#.XyRNSZ5KjIU">Zenodo repository</a>. The notebooks and project code can be found on <a href="https://github.com/htrc/ACS-krewson">GitHub</a>. For more detailed usage information, consult the README files for these repositories.</p>
<div id="project-assets" class="section level2">
<h2>Project assets</h2>
<p>The following table gives a basic sense of the amount of data processed.</p>
<table>
<thead>
<tr class="header">
<th align="left">Project statistics</th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Total volumes processed</td>
<td align="left">183,553</td>
</tr>
<tr class="even">
<td align="left"><strong>Total candidate pages</strong></td>
<td align="left">1,922,602 (685+ GB)</td>
</tr>
<tr class="odd">
<td align="left">Format = JP2</td>
<td align="left">1,901,456</td>
</tr>
<tr class="even">
<td align="left">Format = TIFF</td>
<td align="left">21,269</td>
</tr>
<tr class="odd">
<td align="left">Label = <code>inline_image</code></td>
<td align="left">1,077,544</td>
</tr>
<tr class="even">
<td align="left">Label = <code>plate_image</code></td>
<td align="left">845,181</td>
</tr>
<tr class="odd">
<td align="left"><strong>Total ROIs (JPEG)</strong></td>
<td align="left">2,584,888 (553+ GB)</td>
</tr>
<tr class="even">
<td align="left"><strong>Total ROI vectors (ndarray)</strong></td>
<td align="left">2,584,888 (15+ GB)</td>
</tr>
</tbody>
</table>
<p>The following table describes the project files hosted on Zenodo: https://zenodo.org/record/3940528. At this time, image assets are not publicly available.</p>
<table>
<thead>
<tr class="header">
<th align="left">File</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>google_ids_1800-1850.txt.gz</code></td>
<td align="left">A subset of the July 2019 Hathifile containing all volumes 1800-1850 that were digitized by Google. These volumes were the basis for all subsequent steps.</td>
</tr>
<tr class="even">
<td align="left"><code>hathi_field_list.txt</code></td>
<td align="left">The Hathifile column names</td>
</tr>
<tr class="odd">
<td align="left"><code>stage1_fastai-retrained-cnn.pkl</code></td>
<td align="left">A convolutional neural network (CNN) retrained with 19C page images. Given a candidate page, returns the likeliest of 10 class labels. Only images labeled <code>inline_image</code> or <code>plate_image</code> were retained in Stage 1.</td>
</tr>
<tr class="even">
<td align="left"><code>stage2_mask-rcnn-bbox-weights.h5</code></td>
<td align="left">A Mask-RCNN model developed by Matterport. The model was retrained with page images for which illustrated regions were annotated with a bounding box. Given an image, the model predicts regions of interest (ROIs) that are likely to contain illustrations. These ROI bounding boxes are used to crop the input image.</td>
</tr>
<tr class="odd">
<td align="left"><code>roi-vectors.tar</code></td>
<td align="left">1000-dimensional <code>numpy</code> arrays (<code>*.npy</code>) representing the cropped images (ROIs) from Stage 2. These vector representations were derived using InceptionV3, a standard image classification CNN. Their shape is <code>(1,1000)</code>.</td>
</tr>
<tr class="even">
<td align="left"><code>early-19C-illustrations_metadata.csv</code></td>
<td align="left">Each row of this summary table corresponds to one of the 2,584,888 ROI crops. The fields are: <code>htid</code>, <code>page_seq</code>, <code>page_label</code>, <code>crop_no</code>, <code>vector_path</code>.This is allows easy browsing of a given page on Hathitrust: <code>https://babel.hathitrust.org/cgi/pt?id=&lt;htid&gt;&amp;view=1up&amp;seq=&lt;page_seq&gt;</code>. The <code>crop_no</code> field reflects the possibility that a page could have multiple ROIs on it.</td>
</tr>
<tr class="odd">
<td align="left"><code>early-19C-illustrations_full-index_list.txt.gz</code></td>
<td align="left">A list of all vector files used in the creation of the full-dataset Annoy nearest neighbors index. The order in this file provides the integer indices from <code>0</code>to <code>n-1</code> for each vector in the <code>.ann</code> index.</td>
</tr>
<tr class="even">
<td align="left"><code>early-19C-illustrations_full-index.ann</code></td>
<td align="left">A memory-mapped Annoy nearest neighbors index created from <code>early-19C-illustrations_full-index_list.txt.gz</code>. Should be accessed with<code>AnnoyIndex(1000, 'angular')</code> to match the dimension and metric parameters from when it was built. The index was built with 100 trees and is very fast for finding a reasonable number of neighboring vectors (&lt;100 works well).</td>
</tr>
<tr class="odd">
<td align="left"><code>pixplot-metadata_munroe-francis.csv</code></td>
<td align="left">A metadata file derived by searching <code>google_ids_1800-1850.txt.gz</code> for publishers (the <code>imprint</code> field) matching the firm “Munroe [and] Francis” (and variants). For the 360 matching <code>htids</code>, 1477 ROI crops existed. While the image data is not available through Zenodo, the CSV contains the <code>*.jpg</code> filenames that follow the project’s conventions. The fields conform to those used by the PixPlot viewer: <code>filename</code> (path to image file), <code>label</code> (name of the subset: in this case, “munroe-francis”), <code>description</code> (volume title), <code>year</code> (volume year of publication).</td>
</tr>
</tbody>
</table>
<p>The following notebooks are available via the project’s code repository: https://github.com/htrc/ACS-krewson:</p>
<ul>
<li><code>find_page_neighbors.ipynb</code> — Given a <code>htid</code> and page sequence number, return metadata for the <code>k</code> most similar images in the project dataset. This method is useful when browsing HathiTrust, since the <code>htid</code> and <code>page_seq</code> arguments are displayed in the viewer URL. Any input pages must have been processed by the project. Future methods for out-of-dataset inputs are planned (but these require extracting and vectorizing the input page image).</li>
<li><code>visualize_query.ipynb</code> — Query the Hathifile subset used by the project (e.g. search the <code>imprint</code> field for a particular publisher) and reformat the results into metadata that can be used by the PixPlot visualizer. Code for building a small nearest-neighbors index is also provided. In many cases, it is more useful to run similarity comparisons on small portions of the data — for instance, the illustration styles used by different publishing houses over time.</li>
</ul>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<p>Discussion.</p>
<div class="figure">
<img src="./munroe-francis_montage_square.png" />

</div>
<p>There are several applications for this dataset…</p>
<ul>
<li>Reverse image search</li>
</ul>
<p>Cite Melvers and Smit and some other papers from MHL.</p>
</div>
<div id="acknowledgements" class="section level2">
<h2>Acknowledgements</h2>
<p>That this project was successfully completed is due to the following individuals. Through a year of many changes, and my own lapses in concentration, they stuck with me and are fully responsible for a really nice result.</p>
<ul>
<li>Ryan Dubnicek, for his empathetic, and calm project management</li>
<li>Boris Capitanu for his technical abilities and good humor and willingness to</li>
<li>Eleanor Dickson Koehl, for perceptive questions about the project’s place in the wider world of DH research</li>
<li>Doug Duhaime for use of his vectorization code and generous advice. As well as for bringing this grant to my attention.</li>
</ul>
<div id="refs" class="references">

</div>
</div>
</div>
</body>
</html>
