{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early 19C Illustration Metadata: Final Report\n",
    "\n",
    "The my ACS project has successfully concluded with the creation of a large and novel dataset of illustration metadata. The dataset was produced in four stages using two specially retrained convolutional neural networks as well as one standard model (InceptionV3).\n",
    "\n",
    "The key deliverables of this projects are the following:\n",
    "\n",
    "- A csv file identifying all illustrated pages from HathiTrust volumes from the early 19C\n",
    "- A nearest-neighbors index (and utilities) for finding similar images to a \n",
    "\n",
    "I will discuss the four stages briefly before turning to a discussion and some examples. All listed files are included in the project's [Zenodo repository](TODO) unless stated otherwise. \n",
    "\n",
    "## Classification\n",
    "\n",
    "We began by identifying all Google-digitized volumes published during the years 1800-1850 (inclusive). These **500,013** volumes are contained in the file `google_ids_1800-1850.txt.gz`, which is a subset of the July 2019 [Hathifile](https://www.hathitrust.org/hathifiles). The Hathifile fields include basic publication information and are listed in `hathi_field_list.txt`.\n",
    "\n",
    "From this comprehensive set of early-nineteenth century volumes, we find all potentially illustrated pages using OCR-derived metadata. Apply a retrained CNN model to filter out noisy candidate pages.\n",
    "\n",
    "This model is built with Tensorflow and is located here: `model1`. Code for interacting with the model is here.\n",
    "\n",
    "My midpoint report describes the early steps in greater detail and can be found [here](https://wiki.htrc.illinois.edu/display/COM/A+Half-Century+of+Illustrated+Pages%3A+ACS+Lab+Notes).\n",
    "\n",
    "## Region of interest (ROI) extraction\n",
    "\n",
    "There **2,584,888** total ROIs. \n",
    "\n",
    "## Dimensionality reduction\n",
    "## Indexing and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Talk about users and applications as well as challenges (so much time for indexing steps). Future work. Need for image infrastructure and distributed workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the data\n",
    "\n",
    "This section presents Python code for working with the dataset. Note that HathiTrust APIs are used only sparingly. In general, it is much more efficient to download metadata in bulk and parse those files instead of making API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random, re, sys\n",
    "from annoy import AnnoyIndex\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try the full index... it is VERY fast!\n",
    "# It's just that build the index is linear: https://markroxor.github.io/gensim/static/notebooks/annoytutorial.html\n",
    "u2 = AnnoyIndex(f, 'angular')\n",
    "u2.load('../_app-files/early-19C-illustrations_full-index.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('..\\_app-files\\early-19C-illustrations_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['htid', 'page_seq', 'page_label', 'crop_no', 'vector_path'], dtype='object')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htid_page_seq_nns_metadata(htid, seq_num, nns_index, df_meta, k):\n",
    "    \"\"\"Given a target htid:page_seq and metadata dataframe, gives metadata for k neighboring images\"\"\"\n",
    "    \n",
    "    # get the index for the page in question (N.B. whitespace in column name)\n",
    "    idx = df_meta[(df_meta['htid'] == htid) & (df_meta['page_seq'] == seq_num)].index\n",
    "    \n",
    "    # multiple crops alert\n",
    "    if len(idx) > 1:\n",
    "        print(\"Multiple crops for this page_seq\")\n",
    "        \n",
    "    # the nearest neighbor ROI indices\n",
    "    nns = nns_index.get_nns_by_item(idx[0], k)\n",
    "    \n",
    "    print(nns)\n",
    "    \n",
    "    # return rows from metadat table matching these indices\n",
    "    return df_meta.iloc[nns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1617907, 254046, 1613603, 1269805, 1844016, 1341309, 1724782, 1407896, 2438718, 1454741]\n",
      "                        htid  page_seq    page_label  crop_no  \\\n",
      "1617907  uiug.30112003448526        28  inline_image        0   \n",
      "254046        ucm.5321309033       347  inline_image        0   \n",
      "1613603  uiug.30112048888058       326  inline_image        0   \n",
      "1269805        chi.097881099        75  inline_image        0   \n",
      "1844016       uc1.c046857802       166  inline_image        1   \n",
      "1341309         chi.79355181       551  inline_image        0   \n",
      "1724782         uc1.$b557159       190   plate_image        0   \n",
      "1407896   njp.32101063578338        46   plate_image        0   \n",
      "2438718           hvd.hn5cxz        19  inline_image        0   \n",
      "1454741   njp.32101080155110        80   plate_image        0   \n",
      "\n",
      "                                            vector_path  \n",
      "1617907  uiug/31042/uiug.30112003448526_00000028_00.npy  \n",
      "254046          ucm/5193/ucm.5321309033_00000347_00.npy  \n",
      "1613603  uiug/31485/uiug.30112048888058_00000326_00.npy  \n",
      "1269805           chi/080/chi.097881099_00000075_00.npy  \n",
      "1844016         uc1/c672/uc1.c046857802_00000166_01.npy  \n",
      "1341309            chi/758/chi.79355181_00000551_00.npy  \n",
      "1724782            uc1/$55/uc1.$b557159_00000190_00.npy  \n",
      "1407896    njp/30673/njp.32101063578338_00000046_00.npy  \n",
      "2438718               hvd/hc/hvd.hn5cxz_00000019_00.npy  \n",
      "1454741    njp/30851/njp.32101080155110_00000080_00.npy  \n"
     ]
    }
   ],
   "source": [
    "# explain how this stuff can be found through HT viewer! it's in the URL\n",
    "# end writeup with rousing call for this metadata to be derived for ALL hathi years + every time a new\n",
    "# set of scans gets uploaded\n",
    "univ_history = htid_page_seq_nns_metadata('uiug.30112003448526', 28, u2, df_meta, 10)\n",
    "print(univ_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: ask Boris if htids are encoded with the standard utility methods\n",
    "# In the meta_csv, htids are NOT encoded; in vectors.txt and anywhere they are in file format, they ARE encoded\n",
    "\n",
    "\n",
    "# find way to look up TITLE of work\n",
    "# do montage of ENTIRE publisher set (really want t-SNE, but maybe Damon has something like that?)\n",
    "# don't even need vectors, per se, for t-SNE on tractable subsets\n",
    "# compare Hawthorne ballon (my McNeil paper) with Parley's moon and stars (sadly, added too late to make project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Thank you Ryan and Boris, especially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Project assets\n",
    "\n",
    "The deliverables are available via Zenodo:\n",
    "\n",
    "- A csv file with basic metadata\n",
    "- A nearest-neighbors index\n",
    "\n",
    " The code used for the data processing is on [GitHub](https://github.com/htrc/ACS-krewson)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
